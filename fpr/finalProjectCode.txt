testorb.py-------------

import cv2
from time import time
import glob
import os
import logging
import numpy as np

class ImageLoader(object):
    def __init__(self, filepath: str):
        self.images = (
            glob.glob(os.path.join(filepath, "*.png"))
            + glob.glob(os.path.join(filepath, "*.jpg"))
            + glob.glob(os.path.join(filepath, "*.ppm"))
        )
        self.images.sort()
        self.num_images = len(self.images)
        logging.info("Loading %s images", {self.num_images})
        self.mode = "images"

    def __getitem__(self, item):
        filename = self.images[item]
        img = cv2.imread(filename)
        return img

    def __len__(self):
        return self.num_images

def measure(orb_extractor, image_loader) :
    timings = []
    print("Runing benchmark on {} images".format(len(image_loader)))
    for image in image_loader:
        #  image: (H, W, C)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        start_time = time()
        kpts, dess = orb.detectAndCompute(image,None)
        end_time = time()
        duration = (end_time - start_time) * 1000  # convert to ms.
        timings.append(duration)

    print(f"mean: {np.mean(timings):.2f}ms")
    print(f"median: {np.median(timings):.2f}ms")
    print(f"min: {np.min(timings):.2f}ms")
    print(f"max: {np.max(timings):.2f}ms")

def keypoints_to_img(keypoints,img_tensor):
    # use if keypoints is a numpy array
    _, _, h, w = img_tensor.shape
    wh = np.array([w - 1, h - 1], dtype=np.float32)

    keypoints = wh * (keypoints + 1) / 2
    return keypoints

if __name__ == "__main__":
    orb = cv2.ORB_create()
    dataset_path={"MH01":"/home/nembot/datasets/MH01","MH02":"/home/nembot/datasets/MH02","V101":"/home/nembot/datasets/V101","V201":"/home/nembot/datasets/V201"}

    for _ in range(5):
        for name,path in dataset_path.items():
            image_path=path+"/mav0/cam0/data"
            image_loader = ImageLoader(image_path)
            print("Testing ORB Feature Extractor on {} dataset".format(name))
            measure(orb, image_loader)
            
testkeypoint.py
            
         import cv2
from time import time
import glob
import os
import logging
import numpy as np

class ImageLoader(object):
    def __init__(self, filepath: str):
        self.images = (
            glob.glob(os.path.join(filepath, "*.png"))
            + glob.glob(os.path.join(filepath, "*.jpg"))
            + glob.glob(os.path.join(filepath, "*.ppm"))
        )
        self.images.sort()
        self.num_images = len(self.images)
        logging.info("Loading %s images", {self.num_images})
        self.mode = "images"

    def __getitem__(self, item):
        filename = self.images[item]
        img = cv2.imread(filename)
        return img

    def __len__(self):
        return self.num_images

def measure(orb_extractor, image_loader) :
    keypoint_count = []
    print("Runing benchmark on {} images".format(len(image_loader)))
    for image in image_loader:
        #  image: (H, W, C)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        kpts, dess = orb.detectAndCompute(image,None)
        keypoint_count.append(len(kpts))

    print(f"mean: {np.mean(keypoint_count):.2f} keypoints")
    print(f"median: {np.median(keypoint_count):.2f} keypoints")
    print(f"min: {np.min(keypoint_count):.2f} keypoints")
    print(f"max: {np.max(keypoint_count):.2f} keypoints")

def keypoints_to_img(keypoints,img_tensor):
    # use if keypoints is a numpy array
    _, _, h, w = img_tensor.shape
    wh = np.array([w - 1, h - 1], dtype=np.float32)

    keypoints = wh * (keypoints + 1) / 2
    return keypoints

if __name__ == "__main__":
    orb = cv2.ORB_create()
    dataset_path={"MH01":"/home/nembot/datasets/MH01","MH02":"/home/nembot/datasets/MH02","V101":"/home/nembot/datasets/V101","V201":"/home/nembot/datasets/V201"}

    for _ in range(5):
        for name,path in dataset_path.items():
            image_path=path+"/mav0/cam0/data"
            image_loader = ImageLoader(image_path)
            print("Testing ORB Feature Extractor on {} dataset".format(name))
            measure(orb, image_loader)

testaliked.py-------------

import cv2
from tqdm import tqdm
from time import time
import numpy as np
import glob
import os
import logging
from nets.aliked import ALIKED
import warnings
warnings.filterwarnings("ignore")

class ImageLoader(object):
    def __init__(self, filepath: str):
        self.images = (
            glob.glob(os.path.join(filepath, "*.png"))
            + glob.glob(os.path.join(filepath, "*.jpg"))
            + glob.glob(os.path.join(filepath, "*.ppm"))
        )
        self.images.sort()
        self.num_images = len(self.images)
        logging.info("Loading %s images", {self.num_images})
        self.mode = "images"

    def __getitem__(self, item):
        filename = self.images[item]
        img = cv2.imread(filename)
        return img

    def __len__(self):
        return self.num_images

def measure(aliked_model, image_loader) :
    print("Runing benchmark on {} images".format(len(image_loader)))
    timings = []
    for image in tqdm(image_loader):
        #  image: (H, W, C)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        start_time = time()
        keypoints, scores, descriptors = aliked_model.run(image)
        end_time = time()
        duration = (end_time - start_time) * 1000  # convert to ms.
        timings.append(duration)

    print(f"mean: {np.mean(timings):.2f}ms")
    print(f"median: {np.median(timings):.2f}ms")
    print(f"min: {np.min(timings):.2f}ms")
    print(f"max: {np.max(timings):.2f}ms")
def keypoints_to_img(keypoints,img_tensor):
    # use if keypoints is a numpy array
    _, _, h, w = img_tensor.shape
    wh = np.array([w - 1, h - 1], dtype=np.float32)

    keypoints = wh * (keypoints + 1) / 2
    return keypoints

if __name__ == "__main__":
    dataset_path = {"MH01": "/home/nembot/datasets/MH01", "MH02": "/home/nembot/datasets/MH02",
                    "V101": "/home/nembot/datasets/V101", "V201": "/home/nembot/datasets/V201"}

    for _ in range(5):
        for name, path in dataset_path.items():
            print("Testing pytorch ALIKED Feature Extractor on {} dataset".format(name))
            image_path = path + "/mav0/cam0/data"
            image_loader = ImageLoader(image_path)
            model = ALIKED(model_name="aliked-n32", top_k=1000)
            measure(model,image_loader)
            
testkeypointaliked.py-------------

import cv2
from tqdm import tqdm
import numpy as np
import glob
import os
import logging
from nets.aliked import ALIKED
import warnings
warnings.filterwarnings("ignore")

class ImageLoader(object):
    def __init__(self, filepath: str):
        self.images = (
            glob.glob(os.path.join(filepath, "*.png"))
            + glob.glob(os.path.join(filepath, "*.jpg"))
            + glob.glob(os.path.join(filepath, "*.ppm"))
        )
        self.images.sort()
        self.num_images = len(self.images)
        logging.info("Loading %s images", {self.num_images})
        self.mode = "images"

    def __getitem__(self, item):
        filename = self.images[item]
        img = cv2.imread(filename)
        return img

    def __len__(self):
        return self.num_images

def measure(aliked_model, image_loader) :
    print("Runing benchmark on {} images".format(len(image_loader)))
    keypoint_count = []
    for image in tqdm(image_loader):
        #  image: (H, W, C)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        keypoints = aliked_model.run(image)['keypoints']
        keypoint_count.append(len(keypoints))
        # print(len(keypoints))


    print(f"mean: {np.mean(keypoint_count):.2f} keypoints")
    print(f"median: {np.median(keypoint_count):.2f} keypoints")
    print(f"min: {np.min(keypoint_count):.2f} keypoints")
    print(f"max: {np.max(keypoint_count):.2f} keypoints")
def keypoints_to_img(keypoints,img_tensor):
    # use if keypoints is a numpy array
    _, _, h, w = img_tensor.shape
    wh = np.array([w - 1, h - 1], dtype=np.float32)

    keypoints = wh * (keypoints + 1) / 2
    return keypoints

if __name__ == "__main__":
    dataset_path = {"MH01": "/home/nembot/datasets/MH01", "MH02": "/home/nembot/datasets/MH02",
                    "V101": "/home/nembot/datasets/V101", "V201": "/home/nembot/datasets/V201"}

    for _ in range(5):
        for name, path in dataset_path.items():
            print("Testing pytorch ALIKED Feature Extractor on {} dataset".format(name))
            image_path = path + "/mav0/cam0/data"
            image_loader = ImageLoader(image_path)
            model = ALIKED(model_name="aliked-n32", top_k=-1,scores_th=0.05)
            measure(model,image_loader)
            
testonnx.py-------------

import cv2
from torchvision.transforms import ToTensor
import onnxruntime
import onnx
from tqdm import tqdm
from time import time
import torch
import numpy as np
import glob
import os
import logging

class ImageLoader(object):
    def __init__(self, filepath: str):
        self.images = (
            glob.glob(os.path.join(filepath, "*.png"))
            + glob.glob(os.path.join(filepath, "*.jpg"))
            + glob.glob(os.path.join(filepath, "*.ppm"))
        )
        self.images.sort()
        self.num_images = len(self.images)
        logging.info("Loading %s images", {self.num_images})
        self.mode = "images"

    def __getitem__(self, item):
        filename = self.images[item]
        img = cv2.imread(filename)
        return img

    def __len__(self):
        return self.num_images

def measure(ort_session, image_loader) :
    print("Runing benchmark on {} images".format(len(image_loader)))
    timings = []
    for image in tqdm(image_loader):
        #  image: (H, W, C)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image_tensor = (
            ToTensor()(image).to("cuda").unsqueeze(0)
        )
        input_data = [image_tensor]
        defined_inputs = ort_session.get_inputs()
        ort_inputs = {
            defined_inputs[index].name: to_numpy(input_data[index])
            for index in range(len(defined_inputs))
        }

        start_time = time()

        pred_onnx = ort_session.run(
            [
                "keypoints",
                "descriptors",
                "scores",
            ],
            ort_inputs,
        )

        end_time = time()
        duration = (end_time - start_time) * 1000  # convert to ms.
        timings.append(duration)

    print(f"mean: {np.mean(timings):.2f}ms")
    print(f"median: {np.median(timings):.2f}ms")
    print(f"min: {np.min(timings):.2f}ms")
    print(f"max: {np.max(timings):.2f}ms")

def to_numpy(tensor: torch.Tensor):
    return (
        tensor.detach().cpu().numpy()
        if tensor.requires_grad
        else tensor.cpu().numpy()
    )

def keypoints_to_img(keypoints,img_tensor):
    # use if keypoints is a numpy array
    _, _, h, w = img_tensor.shape
    wh = np.array([w - 1, h - 1], dtype=np.float32)

    keypoints = wh * (keypoints + 1) / 2
    return keypoints

if __name__ == '__main__':
    onnx_model_path = "./converted_onnx_models/aliked-n32-top1k-euroc.onnx"
    dataset_path = {"MH01": "/home/nembot/datasets/MH01", "MH02": "/home/nembot/datasets/MH02",
                    "V101": "/home/nembot/datasets/V101", "V201": "/home/nembot/datasets/V201"}

    for _ in range(5):
        for name, path in dataset_path.items():
            print("Testing Onnx ALIKED Feature Extractor on {} dataset".format(name))
            image_path = path + "/mav0/cam0/data"
            image_loader = ImageLoader(image_path)
            ort_session = onnxruntime.InferenceSession(onnx_model_path)
            measure(ort_session,image_loader)
            
testtrt.py-------------

from trt_model import LOGGER_DICT, TRTInference
import cv2
from time import time
from torchvision.transforms import ToTensor
import numpy as np
import glob
import os
import logging
from tqdm import tqdm

class ImageLoader(object):
    def __init__(self, filepath: str):
        self.images = (
            glob.glob(os.path.join(filepath, "*.png"))
            + glob.glob(os.path.join(filepath, "*.jpg"))
            + glob.glob(os.path.join(filepath, "*.ppm"))
        )
        self.images.sort()
        self.num_images = len(self.images)
        logging.info("Loading %s images", {self.num_images})
        self.mode = "images"

    def __getitem__(self, item):
        filename = self.images[item]
        img = cv2.imread(filename)
        return img

    def __len__(self):
        return self.num_images

def measure(engine, image_loader) :
    print("Runing benchmark on {} images".format(len(image_loader)))
    timings = []
    for image in tqdm(image_loader):
        #  image: (H, W, C)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image_tensor = ToTensor()(image).unsqueeze(0)  # (B, C, H, W)
        image = image_tensor.numpy()

        start_time = time()
        engine.infer(image)
        end_time = time()
        duration = (end_time - start_time) * 1000  # convert to ms.
        timings.append(duration)

    print(f"mean: {np.mean(timings):.2f}ms")
    print(f"median: {np.median(timings):.2f}ms")
    print(f"min: {np.min(timings):.2f}ms")
    print(f"max: {np.max(timings):.2f}ms")

def keypoints_to_img(keypoints,img_tensor):
   # use if keypoints is a numpy array
    _, _, h, w = img_tensor.shape
    wh = np.array([w - 1, h - 1], dtype=np.float32)

    keypoints = wh * (keypoints + 1) / 2
    return keypoints
if __name__ == '__main__':
    trt_model_path ="./converted_trt_models/fp16-aliked-n32-top1k-euroc.onnx"# "converted_trt_models/aliked-n32-top1k-euroc.trt"
    dataset_path = {"MH01": "/home/nembot/datasets/MH01", "MH02": "/home/nembot/datasets/MH02",
                    "V101": "/home/nembot/datasets/V101", "V201": "/home/nembot/datasets/V201"}
    warmup_img_loc = "./assets/euroc/1403636620963555584.png"  # "/home/nembot/Datasets/EuRoc/MH01/mav0/cam0/data/1403636642563555584.png"
    trt_logger = LOGGER_DICT["verbose"]
    model_name = "aliked-n32"
    engine = TRTInference(trt_model_path, model_name, trt_logger)

    #Warming up TRT Engine
    warmup_image = cv2.imread(warmup_img_loc)
    warmup_image = cv2.cvtColor(warmup_image, cv2.COLOR_BGR2RGB)
    print("Starting warm-up ...")
    image_tensor = ToTensor()(warmup_image).unsqueeze(0)  # (B, C, H, W)
    image = image_tensor.numpy()
    image_w = image
    num_iterations = 100  # number of iterartions to warmup the model
    for _ in range(num_iterations):
        engine.infer(image_w)
    print("Warm-up done!")

    # for _ in range(5):
    for name, path in dataset_path.items():
        print("Testing TensorRT ALIKED Feature Extractor on {} dataset".format(name))
        image_path = path + "/mav0/cam0/data"
        image_loader = ImageLoader(image_path)
        measure(engine, image_loader)
        
trt_model.py-------------

from typing import Any
import numpy as np
import pycuda.autoinit
import pycuda.driver as cuda
import tensorrt as trt

from torchvision.transforms import ToTensor
from nets.aliked import ALIKED_CFGS


LOGGER_DICT = {
    "warning": trt.Logger(trt.Logger.WARNING),
    "info": trt.Logger(trt.Logger.INFO),
    "verbose": trt.Logger(trt.Logger.VERBOSE),
}


class TRTInference:
    def __init__(
        self,
        trt_engine_path: str,
        model_type: str,
        trt_logger: trt.Logger,
    ):
        self.trt_logger = trt_logger

        # get configurations
        _, _, _, _, self.dim, _, _ = [
            v for _, v in ALIKED_CFGS[model_type].items()
        ]

        self.cfx = cuda.Device(0).make_context()
        stream = cuda.Stream()

        trt.init_libnvinfer_plugins(self.trt_logger, "")
        runtime = trt.Runtime(self.trt_logger)

        # deserialize engine
        with open(trt_engine_path, "rb") as trt_engine_file:
            buf = trt_engine_file.read()
            engine = runtime.deserialize_cuda_engine(buf)
        context = engine.create_execution_context()

        # prepare buffer
        host_inputs = []
        cuda_inputs = []
        host_outputs = []
        cuda_outputs = []

        tensor_names = [
            engine.get_tensor_name(index)
            for index in range(engine.num_io_tensors)
        ]
        tensor_name = engine.get_tensor_name(0)  # input tensor
        context.set_input_shape(tensor_name,(1, 3, 480, 752) )  # use your input_shape
        assert context.all_binding_shapes_specified
        for tensor_name in tensor_names:
            if engine.get_tensor_mode(tensor_name) == trt.TensorIOMode.INPUT:
                context.set_input_shape(tensor_name, (1, 3, 480, 752))  # use your input_shape
                assert context.all_binding_shapes_specified
            msg = "\n=============================="
            data_type = (
                np.float32
                if engine.get_tensor_dtype(tensor_name) == trt.DataType.FLOAT
                else np.int32
            )
            msg += f"\n{tensor_name}: {data_type}"
            size = trt.volume(engine.get_tensor_shape(tensor_name))
            msg += (
                "\nengine.get_tensor_shape(tensor_name): "
                f"{engine.get_tensor_shape(tensor_name)}"
            )
            msg += f"\nsize: {size}"
            msg += "\n=============================="
            self.trt_logger.log(trt.Logger.INFO, msg)
            host_mem = cuda.pagelocked_empty(size, data_type)
            cuda_mem = cuda.mem_alloc(host_mem.nbytes)
            #Reference : https://forums.developer.nvidia.com/t/tensorrt-v10-inference-using-context-execute-async-v3/289771/2
            context.set_tensor_address(tensor_name, cuda_mem)
            if engine.get_tensor_mode(tensor_name) == trt.TensorIOMode.INPUT:
                host_inputs.append(host_mem)
                cuda_inputs.append(cuda_mem)
            else:  # == trt.TensorIOMode.OUTPUT
                host_outputs.append(host_mem)
                cuda_outputs.append(cuda_mem)

        # store
        self.stream = stream
        self.context = context
        self.engine = engine

        self.host_inputs = host_inputs
        self.cuda_inputs = cuda_inputs
        self.host_outputs = host_outputs
        self.cuda_outputs = cuda_outputs
    
    def warmup(self, image: np.ndarray, num_iterations: int = 3) -> None:
        print("Starting warm-up ...")
        for _ in range(num_iterations):
            self.run(image)
        print("Warm-up done!")
    
    def run(self, image):
        image_tensor = ToTensor()(image).unsqueeze(0)  # (B, C, H, W)
        image = image_tensor.numpy()
        # image = np.expand_dims(image.transpose(2, 0, 1), 0) # (1, C, H, W)
        _, _, h, w = image.shape
        wh = np.array([w - 1, h - 1])
        # print(self.dim)
        keypoints, descriptors, scores, = self.infer(image) #order of output changed based on findings from self.dims
        keypoints = keypoints.reshape(-1, 2)
        keypoints = wh * (keypoints + 1) / 2


        return {
            "keypoints": keypoints.reshape(-1, 2),  # N 2
            "descriptors": descriptors.reshape(-1, self.dim),  # N D
            "scores": scores,  # B N D
        }

    def infer(self, image):
        self.cfx.push()

        # restore
        stream = self.stream
        context = self.context
        engine = self.engine

        host_inputs = self.host_inputs
        cuda_inputs = self.cuda_inputs
        host_outputs = self.host_outputs
        cuda_outputs = self.cuda_outputs

        # Copy data to GPU.
        for index, host_input in enumerate(host_inputs):
            pagelocked_buffer = host_input
            flattened_data = image[index].flatten()
            data_size = flattened_data.shape[0]
            np.copyto(pagelocked_buffer[:data_size], flattened_data)
        for cuda_inp, host_inp in zip(cuda_inputs, host_inputs):
            cuda.memcpy_htod_async(cuda_inp, host_inp, stream)
        # Inference.
        context.execute_async_v3(
            stream_handle=stream.handle
        )

        # Copy to host.
        for cuda_out, host_out in zip(cuda_outputs, host_outputs):
            cuda.memcpy_dtoh_async(host_out, cuda_out, stream)
        stream.synchronize()

        keypoints, scores, descriptors = host_outputs

        self.cfx.pop()
        return keypoints, scores, descriptors

    def __del__(self):
        self.cfx.pop()
